
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>机器学习之线性回归 - easypi's blog</title>
  <meta name="author" content="厚之成">

  
  <meta name="description" content="机器学习是一种广泛应用的技术，其中有监督的学习是其主要内容。本文介绍一类简单的有监督学习算法：线性回归。线性回归是最基础的有监督学习算法，本文将从理论介绍，算法实现，应用实例三个方面对该算法进行介绍。 介绍 有监督学习方法通常具有三个要素：假设模型、优化目标(cost function)、算法。 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://easypi.github.com/blog/2013/04/28/linear-regression-of-machine-regression/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="easypi's blog" type="application/atom+xml">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  

</head>

<body   >
  <header role="banner"><hgroup>
  <div id="logo">
  	<div id="logoLeft">{</div>
  	<div id="logoText">ep</div>
  	<div id="logoRight">}</div>
  	<div class="clear"></div>
  </div>
  <h1><a href="/">easypi's blog</a></h1>
  
    <h2>Knowledge comes from summary and sharing.</h2>
  
  <div class="clear"></div>
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:easypi.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      
        <h1 class="entry-title">机器学习之线性回归</h1>
      
    
    
      <p class="meta">
        








  


<time datetime="2013-04-28T16:56:00+08:00" pubdate data-updated="true">Apr 28<span>th</span>, 2013</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>机器学习是一种广泛应用的技术，其中有监督的学习是其主要内容。本文介绍一类简单的有监督学习算法：线性回归。线性回归是最基础的有监督学习算法，本文将从理论介绍，算法实现，应用实例三个方面对该算法进行介绍。</p>

<!-- more -->

<h2 id="section">介绍</h2>

<p>有监督学习方法通常具有三个要素：假设模型、优化目标(cost function)、算法。</p>

<h3 id="section-1">假设模型</h3>

<p>线性回归的建设模型用 $h(x)$ 表示：</p>

<script type="math/tex; mode=display"> h(x) = \sum_{i=0}^{n}{\theta_i x_i} = \theta^T x </script>

<p>其中 $\theta$ 称为参数或权重， $x$ 为输入或特征，$n$ 为特征个数（不包括$x_0$）</p>

<h3 id="section-2">优化目标</h3>

<p>线性回归的优化目标是使cost function最小，其中cost function 用 $J(\theta)$ 表示， 为</p>

<script type="math/tex; mode=display"> J(\theta) = \frac{1}{2} \sum_{i=1}^{n} (h_{\theta}(x^{(i)})-y^{(i)})^2 </script>

<p>这个函数叫做普通最小二乘函数（ordinary least squares）。</p>

<h3 id="section-3">算法</h3>

<ol>
  <li>
    <p>梯度下降法</p>

    <p>可以使用梯度下降法来求得最小二乘函数的最小值，梯度下降法表示为</p>

<script type="math/tex; mode=display"> \theta_j := \theta_j - \alpha  \frac{\partial}{\partial \theta_j} J(\theta) </script>

    <p>其中$\theta$开始与初始猜测值，并且更新同时作用于所有参数$j= 0, … n$。其中$\alpha$为学习速度，控制每步更新的幅度。</p>

    <p>对于最小二乘函数，可求得其偏导数为：</p>

<script type="math/tex; mode=display"> \frac{\partial}{\partial \theta_j} J(\theta) = (h_\theta (x)-y)x_j</script>

    <p>因此对于一个训练样本，更新规则为：</p>

<script type="math/tex; mode=display">\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta (x^{(i)})) x_j ^{(i)} </script>

    <p>这就是所谓的最小二乘法（Least Mean Squares, LMS）,也叫Widrow-Hoff法。</p>

    <p>当训练样本多于一个时，由此引出两种方法可以处理多个样本的样本集：批梯度下降法(batch gradient descent)和随机梯度下降法或增量梯度下降法(stochastic gradient descent or incremental gradient descent).</p>

    <p>1.1. 批梯度下降法</p>

    <p>Repeat until convergence {</p>

    <p><script type="math/tex"> \theta_j := \theta_j + \alpha \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - h_\theta (x^{(i)})) x_j ^{(i)} </script> (for every $j$)</p>

    <p>}</p>

    <p>该方法每次更新基于所有$m$个训练样本，由于$J$是一个凸二次函数，因此没有局部最小值，只有唯一一个全局最小值，因此该方法保证收敛到（学习速率$\alpha$不太大）全局最小点。缺点是当$m$太大时，每次更新都基于全部样本，成本太高，因此有了随机梯度下降法。</p>

    <p>1.2. 随机梯度下降法</p>

    <p>Loop {</p>

    <p>for i=1 to m, {</p>

    <p><script type="math/tex">\theta_j := \theta_j + \alpha ( y^{(i)} - h_\theta (x^{(i)})) x_j ^{(i)}</script> (for every $j$)</p>

    <p>}</p>

    <p>}</p>

    <p>该算法每次更新基于一个训练样本，因此也叫做增量梯度下降法，当训练样本特别大时，应用批梯度下降法成本太高时可以使用该方法。该方法的缺点是：通常该方法只能找到全局最小点的近似值，当到达最小点附近时可能发生振荡而永远找不到全局最小值，但实际应用中最小值附近的近似值已经够用了。（PS. 实际使用中随着算法的运行可以减小学习速度$\alpha$的值到0附近，来避免一直振荡从而实现收敛）</p>
  </li>
  <li>
    <p>标准方程法(normal equations)</p>

    <p>梯度下降是一种求得$J$最小值的方法，是一种迭代的方法。另一种方法是一种直接求得解析解的方法，叫做标准方程法。数学中我们知道，导数为零时出现极值，对于$J$极值为最小值，为我们所求。基于该思想，通过构造矩阵，可以直接求出$J$最小时的$\theta$值。不推导直接给出结果：</p>

<script type="math/tex; mode=display"> \theta = (X^T X)^{-1} X^T Y</script>

    <p>其中$X$ 为$m-by-n+1$的矩阵，$(x^{(i)})^T$ 为第$i$个训练样本:</p>

<script type="math/tex; mode=display"> X = \left[ \begin{array}{c} (x^{(1)})^T \\ (x^{(2)})^T \\ ... \\ (x^{(m)})^T \end{array} \right]</script>

    <p>其中$Y$ 为$m$维的向量，$y^{(i)}$ 为第$i$个目标值</p>

<script type="math/tex; mode=display"> Y = \left[ \begin{array}{c} y^{(1)} \\ y^{(2)} \\ ... \\ y^{(m)} \end{array} \right]</script>

    <p>使用该方法可以一步计算出所求参数值，不需迭代。但是当矩阵很大时，求矩阵的逆代价较高，而且如果选取的特征是线性相关的，则矩阵的逆不存在，需要使用伪逆代替。</p>

    <p>综上所述，当特征较少且相互独立时可以使用标准方程的方法求得参数，否则使用梯度下降法。使用梯度下降法时如果训练样本较少，使用批梯度下降法，得到全局最小点；否则使用随机梯度下降法，得到近似全局最小点。</p>
  </li>
</ol>

<h3 id="section-4">相关技巧</h3>

<p>在使用线性回归模型时，有两个需要注意的技巧：特征归一化（feature scaling）和学习速度$\alpha$的设定。</p>

<ol>
  <li>
    <p>特征归一化</p>

    <p>当线性回归模型中有多个特征时，如果其中某个特征的取值范围相比其它特征的取值范围大很多，那么这个特征对最后的假设模型中容易产生比较大的影响，为了获得较好的学习效果，需要使各特征的取值范围相似，这个过程称为归一化。其中最直接的一种方法是平均标准化（mean normalization）：</p>

<script type="math/tex; mode=display"> x_j = \frac{x_j - \mu_j}{s_j}</script>

    <p>其中$x_j$为特征，$\mu_j$为特征的平均值，$s_j$为特征的取值范围，通过这一过程，所有特征的取值范围被规范到$[-0.5,0.5]$ 的范围内。</p>
  </li>
  <li>
    <p>学习速度设定</p>

    <p>学习速度$\alpha$决定算法收敛的快慢，当其取值小时，能够保证收敛但收敛太慢；当取值大时，可能出现震荡而不收敛。因此需要选取合适的学习速度。但没有方法直接选取最好的学习速度，只能是收敛性和收敛速度二者的权衡，需要在实验中观察结果来选取合适的$\alpha$值。</p>
  </li>
</ol>

<h2 id="section-5">代码实现</h2>

<p><code>linear_regression.py</code></p>

<pre><code>import numpy as np
import pylab as pl

def mean_normalization(X_set):
    m, n = X_set.shape


    for j in range(1,n):
        x_j = X_set[:,j]
        x_j_mean = np.mean(x_j)
        x_j_range = np.max(x_j) - np.min(x_j)

        x_j = (x_j - x_j_mean) / (x_j_range + np.spacing(1))
        X_set[:,j] = x_j

    return X_set

def normal_equation(X, y, learning_rate = 0):
    theta = np.linalg.pinv(X.T * X) * X.T * y
    return theta

def batch_gradient(X, y, learning_rate = 1e-1):
    epsilon = 1e-3 # converged if error is less than epsilon
    max_iter = 1e4 # max iterator number of loop

    m, n = X.shape
        
    theta = np.asmatrix( np.zeros([n,1]) )
    ite = 0
    abs_err = list()
    abs_err.append(1e5)

    while ite &lt; max_iter and abs_err[-1] &gt; epsilon: 
        for j in range(n):
            theta[j] = theta[j] + learning_rate * 1/m * ( (y - X * theta).T * X[:,j] )

        abs_err.append( np.linalg.norm( X * theta - y ) )
        ite += 1
        print str(ite) + " abs_err: " + str(abs_err[-1])

    return theta

def stochastic_gradient(X, y, learning_rate = 1e-2):
    epsilon = 1e-3 # converged if error is less than epsilon
    max_iter = 1e3 # max iterator number of loop

    m, n = X.shape
    theta = np.asmatrix( np.zeros([n,1]) )
    ite = 0
    abs_err = list()
    abs_err.append(1e5)

    while ite &lt; max_iter and abs_err[-1] &gt; epsilon: 
        for i in range(m):
            for j in range(n):
                theta[j] = theta[j] + learning_rate * ( y[i,:] - X[i,:] * theta) * X[i,j]
       
        ite += 1
        abs_err.append( np.linalg.norm( X.dot(theta) - y ) )
        print str(ite) + " abs_err: " + str(abs_err[-1])


    return theta
    
class LinearRegression(object):
    """Linear regression model:"""
    def __init__(self, feature_scaling = None):
        self.feature_scaling = feature_scaling
        self.theta = None

    def train(self, X_train, y_train, algorithm = normal_equation, learning_rate = 0.01):

        if X_train.shape[0] != y_train.shape[0]:
            print "different size of X and y in training set"
        
        m, n = X_train.shape

        # feature scaling
        if self.feature_scaling is not None:
            X_train = self.feature_scaling(X_train)

        # training parameter
        self.theta = algorithm(X_train, y_train, learning_rate)

        print self.theta
        
    def predict(self, X_test):
        # feature scaling
        if self.feature_scaling is not None:
            X_test= self.feature_scaling(X_test)

        y_test = self.theta.T * X_test
        return y_test
</code></pre>

<h2 id="section-6">例子</h2>

<p>给出两个线性回归的例子，数据可在本文的源码一节找到。</p>

<ol>
  <li>
    <p>一元线性回归例子：</p>

<script type="math/tex; mode=display"> h(x) = \theta_0 + \theta_1 * x_1 </script>

    <p>对此线性函数找到合适的参数。</p>

    <p><code>linear_regression.py</code></p>

    <pre><code> def main():
     datasets = np.genfromtxt('data.txt', delimiter=',')
     X_train = np.matrix (datasets[:, :-1])
     m, n = X_train.shape
     X_train = np.c_[np.ones([m,1]), X_train]
     y_train = np.matrix (datasets[:, -1]).T

     lr = LinearRegression(feature_scaling=None)
     lr.train(X_train, y_train, algorithm = batch_gradient)

     pl.scatter(X_train[:,1], y_train)
     pl.plot(X_train[:,1], X_train * lr.theta, 'r')
     pl.show()



 if __name__ == '__main__':
     main()
</code></pre>

    <p>回归结果如图</p>

    <p><img src="/images/MachineLearning/LinearRegression/image.png" alt="result" /></p>
  </li>
  <li>
    <p>多元线性回归例子</p>

<script type="math/tex; mode=display"> h(x) = \theta_0 + \theta_1 * x_1 + \theta_2 * x_2 </script>

    <p>对此二元函数，找到合适的参数。对比实验，来比较不同梯度下降算法的性能。</p>

    <p><img src="/images/MachineLearning/LinearRegression/batch_gradient.png" alt="批梯度下降法" />
 <img src="/images/MachineLearning/LinearRegression/stochastic_gradient.png" alt="随机梯度下降法" /></p>

    <p>通过二图比较可知，随机梯度下降法算法学习速度快,能够收敛到一个近似的全局最小点。当数据量大时优势尤为明显。 </p>
  </li>
</ol>

<h2 id="section-7">总结</h2>

<p>本文介绍来线性回归的原理以及实现，并通过实例给出回归结果及算法性能比较。通过本文可以了解机器学习中的线性回归算法，对于实际问题是否使用该算法还需要模型选择的知识，对于使用该算法的问题，还需要研究如何选择特征才能够达到好的学习性能，这些问题将在以后的文章中介绍。</p>

<h2 id="section-8">源码</h2>

<p>可在github上找到本文的<a href="https://github.com/easypi/MachineLearning.git">程序源码以及数据文件</a></p>

<h2 id="section-9">参考</h2>

<ul>
  <li><a href="http://cs229.stanford.edu/">stanford machine learning cs229</a></li>
  <li><a href="https://www.coursera.org/course/ml">Coursera: machine learning by Andrew Ng</a></li>
</ul>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">厚之成</span></span>

      








  


<time datetime="2013-04-28T16:56:00+08:00" pubdate data-updated="true">Apr 28<span>th</span>, 2013</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/machinelearning/'>MachineLearning</a>, <a class='category' href='/blog/categories/notes/'>notes</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  
  

  
    
<!--
    <script type="text/javascript" charset="utf-8">
    (function(){
      var _w = 106 , _h = 58;
      var param = {
        url:location.href,
        type:'5',
        count:'1', /**是否显示分享数，1显示(可选)*/
        appkey:'', /**您申请的应用appkey,显示分享来源(可选)*/
        title:'', /**分享的文字内容(可选，默认为所在页面的title)*/
        pic:'', /**分享图片的路径(可选)*/
        ralateUid:'', /**关联用户的UID，分享微博会@该用户(可选)*/
    	language:'zh_cn', /**设置语言，zh_cn|zh_tw(可选)*/
        rnd:new Date().valueOf()
      }
      var temp = [];
      for( var p in param ){
        temp.push(p + '=' + encodeURIComponent( param[p] || '' ) )
      }
      document.write('<iframe allowTransparency="true" frameborder="0" scrolling="no" src="http://hits.sinajs.cn/A1/weiboshare.html?' + temp.join('&') + '" width="'+ _w+'" height="'+_h+'"></iframe>')
    })()
    </script>

<a href="javascript:void(function(){var d=document,e=encodeURIComponent,s1=window.getSelection,s2=d.getSelection,s3=d.selection,s=s1?s1():s2?s2():s3?s3.createRange().text:'',r='http://www.douban.com/recommend/?url='+e(d.location.href)+'&title='+e(d.title)+'&sel='+e(s)+'&v=1',x=function(){if(!window.open(r,'douban','toolbar=0,resizable=1,scrollbars=yes,status=1,width=450,height=330'))location.href=r+'&r=1'};if(/Firefox/.test(navigator.userAgent)){setTimeout(x,0)}else{x()}})()"><img src="http://img2.douban.com/pics/fw2douban2.png" alt="推荐到豆瓣" /></a>

-->

原创文章，版权声明：自由转载-非商用-非衍生-保持署名 | <a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" id="">Creative Commons BY-NC-ND 3.0</a>
<br/>
<br/>

<div class="bshare-custom icon-medium">
  <a title="分享到" href="http://www.bShare.cn/" id="bshare-shareto" class="bshare-more">分享到</a>
  <a title="分享到新浪微博" class="bshare-sinaminiblog"></a>
  <a title="分享到腾讯微博" class="bshare-qqmb"></a>
  <a title="分享到豆瓣" class="bshare-douban"></a>
  <a title="更多平台" class="bshare-more bshare-more-icon more-style-addthis"></a>
  <span class="BSHARE_COUNT bshare-share-count">0</span></div><script type="text/javascript" charset="utf-8" src="http://static.bshare.cn/b/buttonLite.js#style=-1&amp;uuid=&amp;pophcol=2&amp;lang=zh"></script><script type="text/javascript" charset="utf-8" src="http://static.bshare.cn/b/bshareC0.js"></script>


<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" id="UYScript" src="http://v1.uyan.cc/js/iframe.js?UYUserId=0" async=""></script>
<!-- UY END -->

  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left articlenav" href="/blog/2013/04/22/octopress-on-windows/" title="Previous Post: 在windows 7 上部署已有的中文octopress博客">&laquo; 在windows 7 上部署已有的中文octopress博客</a>
      
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2013/04/28/linear-regression-of-machine-regression/">机器学习之线性回归</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/04/22/octopress-on-windows/">在windows 7 上部署已有的中文octopress博客</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/04/02/design-pattern-in-python-behavioral-ones/">设计模式：Python语言实现之行为型模式</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/03/29/design-pattern-in-python-structual-ones/">设计模式：Python语言实现之结构型模式</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/03/15/design-pattern-in-python-creational-ones/">设计模式：Python语言实现之创建型模式</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/easypi">@easypi</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'easypi',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>新浪微博</h1>
  <ul id="weibo">
    <li>

    <iframe width="100%" height="550" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=550&fansRow=0&ptype=1&speed=0&skin=1&isTitle=1&noborder=1&isWeibo=1&isFans=1&uid=1755353035&verifier=6bf885be&dpc=1"></iframe>
    
    </li>
  </ul>
</section>

  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - 厚之成 -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  











</body>
</html>
